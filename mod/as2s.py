# https://qiita.com/morinaga_daichi/items/606dcf39993875965b9a

import re, os
import numpy as np
import chainer
import chainer.functions as F
import chainer.links as L
import MeCab
import numpy as np
import chainer.serializers as serializers

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
path   = os.path.abspath(os.path.dirname(__file__))+'/'

#データをリストにする
# 教師データ
data_path = path+'dataset/as2s_model/corpus.tsv'
data = []
with open(data_path, 'r', encoding='utf-8') as f:
	lines = f.read().split('\n')
for line in lines:
	input_text, target_text = line.split('\t')
	data.append([[input_text],[target_text]])

gpu = -1
if gpu >= 0: # numpyかcuda.cupyか
	xp = chainer.cuda.cupy
	chainer.cuda.get_device(gpu).use()
else:
	xp = np

# データ変換クラスの定義

class DataConverter:

	def __init__(self, batch_col_size):
		"""クラスの初期化

		Args:
			batch_col_size: 学習時のミニバッチ単語数サイズ
		"""
		self.mecab = MeCab.Tagger() # 形態素解析器
		self.vocab = {"<eos>":0, "<unknown>": 1} # 単語辞書
		self.batch_col_size = batch_col_size

	def load(self, data):
		"""学習時に、教師データを読み込んでミニバッチサイズに対応したNumpy配列に変換する

		Args:
			data: 対話データ
		"""
		# 単語辞書の登録
		self.vocab = {"<eos>":0, "<unknown>": 1} # 単語辞書を初期化
		for d in data:
			sentences = [d[0][0], d[1][0]] # 入力文、返答文
			for sentence in sentences:
				sentence_words = self.sentence2words(sentence) # 文章を単語に分解する
				for word in sentence_words:
					if word not in self.vocab:
						self.vocab[word] = len(self.vocab)
		# 教師データのID化と整理
		queries, responses = [], []
		for d in data:
			query, response = d[0][0], d[1][0] #  エンコード文、デコード文
			queries.append(self.sentence2ids(sentence=query, train=True, sentence_type="query"))
			responses.append(self.sentence2ids(sentence=response, train=True, sentence_type="response"))
		self.train_queries = xp.vstack(queries)
		self.train_responses = xp.vstack(responses)

	def sentence2words(self, sentence):
		"""文章を単語の配列にして返却する

		Args:
			sentence: 文章文字列
		"""
		sentence_words = []
		for m in self.mecab.parse(sentence).split("\n"): # 形態素解析で単語に分解する
			w = m.split("\t")[0].lower() # 単語
			if len(w) == 0 or w == "eos": # 不正文字、EOSは省略
				continue
			sentence_words.append(w)
		sentence_words.append("<eos>") # 最後にvocabに登録している<eos>を代入する
		return sentence_words

	def sentence2ids(self, sentence, train=True, sentence_type="query"):
		"""文章を単語IDのNumpy配列に変換して返却する

		Args:
			sentence: 文章文字列
			train: 学習用かどうか
			sentence_type: 学習用でミニバッチ対応のためのサイズ補填方向をクエリー・レスポンスで変更するため"query"or"response"を指定　
		Returns:
			ids: 単語IDのNumpy配列
		"""
		ids = [] # 単語IDに変換して格納する配列
		sentence_words = self.sentence2words(sentence) # 文章を単語に分解する
		for word in sentence_words:
			if word in self.vocab: # 単語辞書に存在する単語ならば、IDに変換する
				ids.append(self.vocab[word])
			else: # 単語辞書に存在しない単語ならば、<unknown>に変換する
				ids.append(self.vocab["<unknown>"])
		# 学習時は、ミニバッチ対応のため、単語数サイズを調整してNumpy変換する
		if train:
			if sentence_type == "query": # クエリーの場合は前方にミニバッチ単語数サイズになるまで-1を補填する
				while len(ids) > self.batch_col_size: # ミニバッチ単語サイズよりも大きければ、ミニバッチ単語サイズになるまで先頭から削る
					ids.pop(0)
				ids = xp.array([-1]*(self.batch_col_size-len(ids))+ids, dtype="int32")
			elif sentence_type == "response": # レスポンスの場合は後方にミニバッチ単語数サイズになるまで-1を補填する
				while len(ids) > self.batch_col_size: # ミニバッチ単語サイズよりも大きければ、ミニバッチ単語サイズになるまで末尾から削る
					ids.pop()
				ids = xp.array(ids+[-1]*(self.batch_col_size-len(ids)), dtype="int32")
		else: # 予測時は、そのままNumpy変換する
			ids = xp.array([ids], dtype="int32")
		return ids

	def ids2words(self, ids):
		"""予測時に、単語IDのNumpy配列を単語に変換して返却する

		Args:
			ids: 単語IDのNumpy配列
		Returns:
			words: 単語の配列
		"""
		words = [] # 単語を格納する配列
		for i in ids: # 順番に単語IDを単語辞書から参照して単語に変換する
			words.append(list(self.vocab.keys())[list(self.vocab.values()).index(i)])
		return words
# モデルクラスの定義

# LSTMエンコーダークラス
class LSTMEncoder(chainer.Chain):

	def __init__(self, vocab_size, embed_size, hidden_size):
		"""Encoderのインスタンス化

		Args:
			vocab_size: 使われる単語の種類数
			embed_size: 単語をベクトル表現した際のサイズ
			hidden_size: 隠れ層のサイズ
		"""
		super(LSTMEncoder, self).__init__(
			xe = L.EmbedID(vocab_size, embed_size, ignore_label=-1),
			eh = L.Linear(embed_size, 4 * hidden_size),
			hh = L.Linear(hidden_size, 4 * hidden_size)
		)

	def __call__(self, x, c, h):
		"""Encoderの計算

		Args:
			x: one-hotな単語
			c: 内部メモリ
			h: 隠れ層
		Returns:
			次の内部メモリ、次の隠れ層
		"""
		e = F.tanh(self.xe(x))
		return F.lstm(c, self.eh(e) + self.hh(h))

# Attention Model + LSTMデコーダークラス
class AttLSTMDecoder(chainer.Chain):
	def __init__(self, vocab_size, embed_size, hidden_size):
		"""Attention ModelのためのDecoderのインスタンス化

		Args:
			vocab_size: 語彙数
			embed_size: 単語ベクトルのサイズ
			hidden_size: 隠れ層のサイズ
		"""
		super(AttLSTMDecoder, self).__init__(
			ye = L.EmbedID(vocab_size, embed_size, ignore_label=-1), # 単語を単語ベクトルに変換する層
			eh = L.Linear(embed_size, 4 * hidden_size), # 単語ベクトルを隠れ層の4倍のサイズのベクトルに変換する層
			hh = L.Linear(hidden_size, 4 * hidden_size), # Decoderの中間ベクトルを隠れ層の4倍のサイズのベクトルに変換する層
			fh = L.Linear(hidden_size, 4 * hidden_size), # 順向きEncoderの中間ベクトルの加重平均を隠れ層の4倍のサイズのベクトルに変換する層
			bh = L.Linear(hidden_size, 4 * hidden_size), # 順向きEncoderの中間ベクトルの加重平均を隠れ層の4倍のサイズのベクトルに変換する層
			he = L.Linear(hidden_size, embed_size), # 隠れ層サイズのベクトルを単語ベクトルのサイズに変換する層
			ey = L.Linear(embed_size, vocab_size) # 単語ベクトルを語彙数サイズのベクトルに変換する層
		)

	def __call__(self, y, c, h, f, b):
		"""Decoderの計算

		Args:
			y: Decoderに入力する単語
			c: 内部メモリ
			h: Decoderの中間ベクトル
			f: Attention Modelで計算された順向きEncoderの加重平均
			b: Attention Modelで計算された逆向きEncoderの加重平均
		Returns:
			語彙数サイズのベクトル、更新された内部メモリ、更新された中間ベクトル
		"""
		e = F.tanh(self.ye(y)) # 単語を単語ベクトルに変換
		c, h = F.lstm(c, self.eh(e) + self.hh(h) + self.fh(f) + self.bh(b)) # 単語ベクトル、Decoderの中間ベクトル、順向きEncoderのAttention、逆向きEncoderのAttentionを使ってLSTM
		t = self.ey(F.tanh(self.he(h))) # LSTMから出力された中間ベクトルを語彙数サイズのベクトルに変換する
		return t, c, h

# Attentionモデルクラス
class Attention(chainer.Chain):
	def __init__(self, hidden_size):
		"""Attentionのインスタンス化
		Args:
			hidden_size: 隠れ層のサイズ
		"""
		super(Attention, self).__init__(
			fh = L.Linear(hidden_size, hidden_size), # 順向きのEncoderの中間ベクトルを隠れ層サイズのベクトルに変換する線形結合層
			bh = L.Linear(hidden_size, hidden_size), # 逆向きのEncoderの中間ベクトルを隠れ層サイズのベクトルに変換する線形結合層
			hh = L.Linear(hidden_size, hidden_size), # Decoderの中間ベクトルを隠れ層サイズのベクトルに変換する線形結合層
			hw = L.Linear(hidden_size, 1), # 隠れ層サイズのベクトルをスカラーに変換するための線形結合層
		)
		self.hidden_size = hidden_size # 隠れ層のサイズを記憶

	def __call__(self, fs, bs, h):
		"""Attentionの計算

		Args:
			fs: 順向きのEncoderの中間ベクトルが記録されたリスト
			bs: 逆向きのEncoderの中間ベクトルが記録されたリスト
			h: Decoderで出力された中間ベクトル
		Returns:
			順向きのEncoderの中間ベクトルの加重平均、逆向きのEncoderの中間ベクトルの加重平均
		"""
		batch_size = h.data.shape[0] # ミニバッチのサイズを記憶
		ws = [] # ウェイトを記録するためのリストの初期化
		sum_w = chainer.Variable(xp.zeros((batch_size, 1), dtype='float32')) # ウェイトの合計値を計算するための値を初期化
		# Encoderの中間ベクトルとDecoderの中間ベクトルを使ってウェイトの計算
		for f, b in zip(fs, bs):
			w = F.tanh(self.fh(f)+self.bh(b)+self.hh(h)) # 順向きEncoderの中間ベクトル、逆向きEncoderの中間ベクトル、Decoderの中間ベクトルを使ってウェイトの計算
			w = F.exp(self.hw(w)) # softmax関数を使って正規化する
			ws.append(w) # 計算したウェイトを記録
			sum_w += w
		# 出力する加重平均ベクトルの初期化
		att_f = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		att_b = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		for f, b, w in zip(fs, bs, ws):
			w /= sum_w # ウェイトの和が1になるように正規化
			# ウェイト * Encoderの中間ベクトルを出力するベクトルに足していく
			att_f += F.reshape(F.batch_matmul(f, w), (batch_size, self.hidden_size))
			att_b += F.reshape(F.batch_matmul(b, w), (batch_size, self.hidden_size))
		return att_f, att_b

# Attention Sequence to Sequence Modelクラス
class AttSeq2Seq(chainer.Chain):
	def __init__(self, vocab_size, embed_size, hidden_size, batch_col_size):
		"""Attention + Seq2Seqのインスタンス化

		Args:
			vocab_size: 語彙数のサイズ
			embed_size: 単語ベクトルのサイズ
			hidden_size: 隠れ層のサイズ
		"""
		super(AttSeq2Seq, self).__init__(
			f_encoder = LSTMEncoder(vocab_size, embed_size, hidden_size), # 順向きのEncoder
			b_encoder = LSTMEncoder(vocab_size, embed_size, hidden_size), # 逆向きのEncoder
			attention = Attention(hidden_size), # Attention Model
			decoder = AttLSTMDecoder(vocab_size, embed_size, hidden_size) # Decoder
		)
		self.vocab_size = vocab_size
		self.embed_size = embed_size
		self.hidden_size = hidden_size
		self.decode_max_size = batch_col_size # デコードはEOSが出力されれば終了する、出力されない場合の最大出力語彙数
		# 順向きのEncoderの中間ベクトル、逆向きのEncoderの中間ベクトルを保存するためのリストを初期化
		self.fs = []
		self.bs = []

	def encode(self, words, batch_size):
		"""Encoderの計算

		Args:
			words: 入力で使用する単語記録されたリスト
			batch_size: ミニバッチのサイズ
		"""
		c = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		h = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		# 順向きのEncoderの計算
		for w in words:
			c, h = self.f_encoder(w, c, h)
			self.fs.append(h) # 計算された中間ベクトルを記録
		# 内部メモリ、中間ベクトルの初期化
		c = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		h = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		# 逆向きのEncoderの計算
		for w in reversed(words):
			c, h = self.b_encoder(w, c, h)
			self.bs.insert(0, h) # 計算された中間ベクトルを記録
		# 内部メモリ、中間ベクトルの初期化
		self.c = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))
		self.h = chainer.Variable(xp.zeros((batch_size, self.hidden_size), dtype='float32'))

	def decode(self, w):
		"""Decoderの計算

		Args:
			w: Decoderで入力する単語
		Returns:
			予測単語
		"""
		att_f, att_b = self.attention(self.fs, self.bs, self.h)
		t, self.c, self.h = self.decoder(w, self.c, self.h, att_f, att_b)
		return t

	def reset(self):
		"""インスタンス変数を初期化する
		"""
		# Encoderの中間ベクトルを記録するリストの初期化
		self.fs = []
		self.bs = []
		# 勾配の初期化
		self.zerograds()

	def __call__(self, enc_words, dec_words=None, train=True):
		"""順伝播の計算を行う関数

		Args:
			enc_words: 発話文の単語を記録したリスト
			dec_words: 応答文の単語を記録したリスト
			train: 学習か予測か
		Returns:
			計算した損失の合計 or 予測したデコード文字列
		"""
		enc_words = enc_words.T
		if train:
			dec_words = dec_words.T
		batch_size = len(enc_words[0]) # バッチサイズを記録
		self.reset() # model内に保存されている勾配をリセット
		enc_words = [chainer.Variable(xp.array(row, dtype='int32')) for row in enc_words] # 発話リスト内の単語をVariable型に変更
		self.encode(enc_words, batch_size) # エンコードの計算
		t = chainer.Variable(xp.array([0 for _ in range(batch_size)], dtype='int32')) # <eos>をデコーダーに読み込ませる
		loss = chainer.Variable(xp.zeros((), dtype='float32')) # 損失の初期化
		ys = [] # デコーダーが生成する単語を記録するリスト
		# デコーダーの計算
		if train: # 学習の場合は損失を計算する
			for w in dec_words:
				y = self.decode(t) # 1単語ずつをデコードする
				t = chainer.Variable(xp.array(w, dtype='int32')) # 正解単語をVariable型に変換
				loss += F.softmax_cross_entropy(y, t) # 正解単語と予測単語を照らし合わせて損失を計算
			return loss
		else: # 予測の場合はデコード文字列を生成する
			for i in range(self.decode_max_size):
				y = self.decode(t)
				y = xp.argmax(y.data) # 確率で出力されたままなので、確率が高い予測単語を取得する
				ys.append(y)
				t = chainer.Variable(xp.array([y], dtype='int32'))
				if y == 0: # EOSを出力したならばデコードを終了する
					break
			return ys
# 学習

# 定数
embed_size = 100
hidden_size = 100
batch_size = 5 # ミニバッチ学習のバッチサイズ数
batch_col_size = 15
epoch_num = 50 # エポック数
N = len(data) # 教師データの数

# 教師データの読み込み
data_converter = DataConverter(batch_col_size=batch_col_size) # データコンバーター
data_converter.load(data) # 教師データ読み込み
vocab_size = len(data_converter.vocab) # 単語数

# モデルの宣言
model = AttSeq2Seq(vocab_size=vocab_size, embed_size=embed_size, hidden_size=hidden_size, batch_col_size=batch_col_size)
opt = chainer.optimizers.Adam()
opt.setup(model)
opt.add_hook(chainer.optimizer.GradientClipping(5))

if gpu >= 0:
	model.to_gpu(gpu)

model.reset()